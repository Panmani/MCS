# -*- coding: utf-8 -*-
"""captioning_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ruotianluo/ImageCaptioning.pytorch/blob/colab/notebooks/captioning_demo.ipynb
"""

# Part of the code is borrowed from https://colab.research.google.com/drive/1vzrxDYB0vxtuUy8KCaGxm--nDCJvyBSg#scrollTo=HRUp0r_-B9N0&forceEdit=true&sandboxMode=true
# Commented out IPython magic to ensure Python compatibility.
# Instal captioning repo and download the transformer pretrained model
# !gdown --id 1KvRXqfVEXbCGj2LapfQ3py6bLjW9FatB
# !gdown --id 1Rsw1XukVghaaQt7TSrjNogu7_8Kp3n7G

"""
pip install git+https://github.com/ruotianluo/ImageCaptioning.pytorch.git
pip install gdown
gdown --id 1VmUzgu0qlmCMqM1ajoOZxOXP3hiC_qlL
gdown --id 1zQe00W02veVYq-hdq5WsPOS3OPkNdq79

pip install yacs
git clone https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark.git
cd vqa-maskrcnn-benchmark
python setup.py build
python setup.py develop

mkdir model_data
wget -O model_data/detectron_model.pth wget https://dl.fbaipublicfiles.com/vilbert-multi-task/detectron_model.pth
wget -O model_data/detectron_model.yaml wget https://dl.fbaipublicfiles.com/vilbert-multi-task/detectron_config.yaml

pip install transformers
pip install git+https://github.com/ruotianluo/meshed-memory-transformer.git
"""

import sys, os, argparse
sys.path.append('vqa-maskrcnn-benchmark')

import yaml
import cv2
import torch
import requests
import numpy as np
import gc
import torch.nn.functional as F
import pandas as pd
import json
from tqdm import tqdm

import torchvision.models as models
import torchvision.transforms as transforms

from PIL import Image
from IPython.display import display, HTML, clear_output
from ipywidgets import widgets, Layout
from io import BytesIO


from maskrcnn_benchmark.config import cfg
from maskrcnn_benchmark.layers import nms
from maskrcnn_benchmark.modeling.detector import build_detection_model
from maskrcnn_benchmark.structures.image_list import to_image_list
from maskrcnn_benchmark.utils.model_serialization import load_state_dict

import captioning
import captioning.utils.misc
import captioning.models


# COCO_PATH = "../VLBERT/data/coco"
VCR_PATH = "../VLBERT/data/vcr/vcr1images"
OUT_DIR = "../vcr_caps"
split = "val"
img_type = ".jpg"


class FeatureExtractor:
  TARGET_IMAGE_SIZE = [448, 448]
  CHANNEL_MEAN = [0.485, 0.456, 0.406]
  CHANNEL_STD = [0.229, 0.224, 0.225]

  def __init__(self):
    # self._init_processors()
    self.detection_model = self._build_detection_model()

  def __call__(self, url):
    with torch.no_grad():
      detectron_features = self.get_detectron_features(url)

    return detectron_features

  def _build_detection_model(self):

      cfg.merge_from_file('model_data/detectron_model.yaml')
      cfg.freeze()

      model = build_detection_model(cfg)
      checkpoint = torch.load('model_data/detectron_model.pth',
                              map_location=torch.device("cpu"))

      load_state_dict(model, checkpoint.pop("model"))

      model.to("cuda")
      model.eval()
      return model

  def get_actual_image(self, image_path):
      if image_path.startswith('http'):
          path = requests.get(image_path, stream=True).raw
      else:
          path = image_path

      return path

  def _image_transform(self, image_path):
      path = self.get_actual_image(image_path)

      img = Image.open(path)
      im = np.array(img).astype(np.float32)
      im = im[:, :, ::-1]
      im -= np.array([102.9801, 115.9465, 122.7717])
      im_shape = im.shape
      im_size_min = np.min(im_shape[0:2])
      im_size_max = np.max(im_shape[0:2])
      im_scale = float(800) / float(im_size_min)
      # Prevent the biggest axis from being more than max_size
      if np.round(im_scale * im_size_max) > 1333:
           im_scale = float(1333) / float(im_size_max)
      im = cv2.resize(
           im,
           None,
           None,
           fx=im_scale,
           fy=im_scale,
           interpolation=cv2.INTER_LINEAR
       )
      img = torch.from_numpy(im).permute(2, 0, 1)
      return img, im_scale


  def _process_feature_extraction(self, output,
                                 im_scales,
                                 feat_name='fc6',
                                 conf_thresh=0.2):
      batch_size = len(output[0]["proposals"])
      n_boxes_per_image = [len(_) for _ in output[0]["proposals"]]
      score_list = output[0]["scores"].split(n_boxes_per_image)
      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]
      feats = output[0][feat_name].split(n_boxes_per_image)
      cur_device = score_list[0].device

      feat_list = []

      for i in range(batch_size):
          dets = output[0]["proposals"][i].bbox / im_scales[i]
          scores = score_list[i]

          max_conf = torch.zeros((scores.shape[0])).to(cur_device)

          for cls_ind in range(1, scores.shape[1]):
              cls_scores = scores[:, cls_ind]
              keep = nms(dets, cls_scores, 0.5)
              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],
                                           cls_scores[keep],
                                           max_conf[keep])

          keep_boxes = torch.argsort(max_conf, descending=True)[:100]
          feat_list.append(feats[i][keep_boxes])
      return feat_list

  def get_detectron_features(self, image_path):
      im, im_scale = self._image_transform(image_path)
      img_tensor, im_scales = [im], [im_scale]
      current_img_list = to_image_list(img_tensor, size_divisible=32)
      current_img_list = current_img_list.to('cuda')
      with torch.no_grad():
          output = self.detection_model(current_img_list)
      feat_list = self._process_feature_extraction(output, im_scales,
                                                  'fc6', 0.2)
      return feat_list[0]

feature_extractor = FeatureExtractor()

infos = captioning.utils.misc.pickle_load(open('infos_trans12-best.pkl', 'rb'))

infos['opt'].vocab = infos['vocab']

model = captioning.models.setup(infos['opt'])
model.cuda()
model.load_state_dict(torch.load('model-best.pth'))

def get_captions(img_feature):
    # Return the 5 captions from beam serach with beam size 5
    return model.decode_sequence(model(img_feature.mean(0)[None], img_feature[None], mode='sample', opt={'beam_size':5, 'sample_method':'beam_search', 'sample_n':5})[0])

# def init_widgets(url):
#   image_text = widgets.Text(
#     description="Image URL", layout=Layout(minwidth="70%")
#   )
#
#   image_text.value = url
#   submit_button = widgets.Button(description="Caption the image!")
#
#   display(image_text)
#   display(submit_button)
#
#   submit_button.on_click(lambda b: on_button_click(
#       b, image_text
#   ))
#
#   return image_text
#
# def on_button_click(b, image_text):
#   clear_output()
#   image_path = feature_extractor.get_actual_image(image_text.value)
#   image = Image.open(image_path)
#
#   captions = '<br>'.join(get_captions(feature_extractor(image_text.value)))
#   init_widgets(image_text.value)
#   display(image)
#
#   display(HTML(captions))
#

# image_text = init_widgets(
#     "http://images.cocodataset.org/train2017/000000505539.jpg"
# )

def generate_captions(img_path):
    captions = get_captions(feature_extractor(img_path))
    return captions

if __name__ == '__main__':

    # img_dir = "{}2017".format(split)
    # ann_filename = '{}2017_annots.json'.format(split)
    # cap_gen_dir = "{}2017_cap_gen".format(split)
    # metadata_dir = "{}2017_meta".format(split)
    # feat_dir = "{}2017_feat".format(split)
    parser = argparse.ArgumentParser()
    parser.add_argument('--split_num', type=int, default=8,
                        help="Number of splits")
    parser.add_argument('--split_idx', type=int, default=0,
                        help="Which split to generate captions on")
    args = parser.parse_args()

    if not os.path.isdir(OUT_DIR):
        os.mkdir(OUT_DIR)

    movie_list = [dI for dI in os.listdir(VCR_PATH) if os.path.isdir(os.path.join(VCR_PATH,dI))]
    frame_list = []
    vcr_captions = {}
    for movie in tqdm(movie_list):
        movie_path = os.path.join(VCR_PATH, movie)
        movie_out_path = os.path.join(OUT_DIR, movie)
        if not os.path.isdir(movie_out_path):
            os.mkdir(movie_out_path)

        frame_list += [os.path.join(movie, fI) for fI in os.listdir(movie_path) if os.path.isfile(os.path.join(movie_path,fI)) and fI.split(".")[-1] == "jpg"]

    print("Found {} images".format(len(frame_list)))
    split_size = len(frame_list) // args.split_num + 1
    if (args.split_idx + 1) * split_size < len(frame_list):
        start_at, end_at = args.split_idx * split_size, (args.split_idx + 1) * split_size
        print("Generate from {} to {}".format(start_at, end_at))
        frame_split_list = frame_list[start_at: end_at]
    else:
        start_at = args.split_idx * split_size
        print("Generate from {} to the end".format(start_at))
        frame_split_list = frame_list[start_at: ]

    for frame in tqdm(frame_split_list):
        frame_path = os.path.join(VCR_PATH, frame)
        # print("Frame:", frame_path)
        if not os.path.isfile(frame_path):
            print("Cannot find dataset images at {}".format(frame_path))
        captions = generate_captions(frame_path)
        # print(captions)

        vcr_captions[frame] = captions
        with open("vcr_captions_{}.json".format(args.split_idx), "w") as output_file:
            json.dump(vcr_captions, output_file)

